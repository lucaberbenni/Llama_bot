[
    {
        "title": "Code Llama: Open Foundation Models for Code",
        "summary": "We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B and 34B parameters each. All models are trained\non sequences of 16k tokens and show improvements on inputs with up to 100k\ntokens. 7B and 13B Code Llama and Code Llama - Instruct variants support\ninfilling based on surrounding content. Code Llama reaches state-of-the-art\nperformance among open models on several code benchmarks, with scores of up to\n53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python\n7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform\nevery other publicly available model on MultiPL-E. We release Code Llama under\na permissive license that allows for both research and commercial use.",
        "link": "http://arxiv.org/abs/2308.12950v2"
    },
    {
        "title": "DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for\n  Hospitalized Patients",
        "summary": "In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is\npivotal, but its assignment process is inefficient. The study introduces\nDRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes\nto enhance DRGs assignment. Utilizing LLaMA as the foundational model and\noptimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge\nsummaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1\nscore of 0.327, a top-1 prediction accuracy of 52.0%, and a macro-averaged Area\nUnder the Curve (AUC) of 0.986, with a maximum input token length of 512. This\nmodel surpassed the performance of prior leading models in DRG prediction,\nshowing a relative improvement of 40.3% and 35.7% in macro-averaged F1 score\ncompared to ClinicalBERT and CAML, respectively. Applied to base DRG and\ncomplication or comorbidity (CC)/major complication or comorbidity (MCC)\nprediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8% and 67.5%,\nrespectively. Additionally, our findings indicate that DRG-LLaMA's performance\ncorrelates with increased model parameters and input context lengths.",
        "link": "http://arxiv.org/abs/2309.12625v2"
    },
    {
        "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention",
        "summary": "We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.",
        "link": "http://arxiv.org/abs/2303.16199v2"
    },
    {
        "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
        "summary": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
        "link": "http://arxiv.org/abs/2304.15010v1"
    },
    {
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
        "summary": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
        "link": "http://arxiv.org/abs/2304.06975v1"
    },
    {
        "title": "BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B",
        "summary": "Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.",
        "link": "http://arxiv.org/abs/2311.00117v1"
    },
    {
        "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
        "summary": "In this work, we present a novel method to tackle the token generation\nchallenge in Vision Language Models (VLMs) for video and image understanding,\ncalled LLaMA-VID. Current VLMs, while proficient in tasks like image captioning\nand visual question answering, face computational burdens when processing long\nvideos due to the excessive visual tokens. LLaMA-VID addresses this issue by\nrepresenting each frame with two distinct tokens, namely context token and\ncontent token. The context token encodes the overall image context based on\nuser input, whereas the content token encapsulates visual cues in each frame.\nThis dual-token strategy significantly reduces the overload of long videos\nwhile preserving critical information. Generally, LLaMA-VID empowers existing\nframeworks to support hour-long videos and pushes their upper limit with an\nextra context token. It is proved to surpass previous methods on most of video-\nor image-based benchmarks. Code is available\nhttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
        "link": "http://arxiv.org/abs/2311.17043v1"
    },
    {
        "title": "LLAMA: The Low-Level Abstraction For Memory Access",
        "summary": "The performance gap between CPU and memory widens continuously. Choosing the\nbest memory layout for each hardware architecture is increasingly important as\nmore and more programs become memory bound. For portable codes that run across\nheterogeneous hardware architectures, the choice of the memory layout for data\nstructures is ideally decoupled from the rest of a program. This can be\naccomplished via a zero-runtime-overhead abstraction layer, underneath which\nmemory layouts can be freely exchanged.\n  We present the Low-Level Abstraction of Memory Access (LLAMA), a C++ library\nthat provides such a data structure abstraction layer with example\nimplementations for multidimensional arrays of nested, structured data. LLAMA\nprovides fully C++ compliant methods for defining and switching custom memory\nlayouts for user-defined data types. The library is extensible with third-party\nallocators.\n  Providing two close-to-life examples, we show that the LLAMA-generated AoS\n(Array of Structs) and SoA (Struct of Arrays) layouts produce identical code\nwith the same performance characteristics as manually written data structures.\nIntegrations into the SPEC CPU\\textsuperscript{\\textregistered} lbm benchmark\nand the particle-in-cell simulation PIConGPU demonstrate LLAMA's abilities in\nreal-world applications. LLAMA's layout-aware copy routines can significantly\nspeed up transfer and reshuffling of data between layouts compared with naive\nelement-wise copying.\n  LLAMA provides a novel tool for the development of high-performance C++\napplications in a heterogeneous environment.",
        "link": "http://arxiv.org/abs/2106.04284v3"
    },
    {
        "title": "Music Understanding LLaMA: Advancing Text-to-Music Generation with\n  Question Answering and Captioning",
        "summary": "Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity\nof large-scale publicly available music datasets with natural language\ncaptions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA),\ncapable of answering music-related questions and generating captions for music\nfiles. Our model utilizes audio representations from a pretrained MERT model to\nextract music features. However, obtaining a suitable dataset for training the\nMU-LLaMA model remains challenging, as existing publicly accessible audio\nquestion answering datasets lack the necessary depth for open-ended music\nquestion answering. To fill this gap, we present a methodology for generating\nquestion-answer pairs from existing audio captioning datasets and introduce the\nMusicQA Dataset designed for answering open-ended music-related questions. The\nexperiments demonstrate that the proposed MU-LLaMA model, trained on our\ndesigned MusicQA dataset, achieves outstanding performance in both music\nquestion answering and music caption generation across various metrics,\noutperforming current state-of-the-art (SOTA) models in both fields and\noffering a promising advancement in the T2M-Gen research field.",
        "link": "http://arxiv.org/abs/2308.11276v1"
    },
    {
        "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca",
        "summary": "Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. GitHub repository: https://github.com/ymcui/Chinese-LLaMA-Alpaca",
        "link": "http://arxiv.org/abs/2304.08177v2"
    }
]